{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from polnet.utils import *\n",
    "from polnet import lio\n",
    "from polnet import tem\n",
    "from polnet import poly as pp\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "from polnet.stomo import (\n",
    "    SynthTomo,\n",
    "    SetTomos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_absolute_paths(parent_dir):\n",
    "    \"\"\"\n",
    "    Get absolute paths of all directories inside a given directory.\n",
    "    \n",
    "    Parameters:\n",
    "        parent_dir (str): Path to the parent directory.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of absolute paths of subdirectories.\n",
    "    \"\"\"\n",
    "    return [os.path.abspath(os.path.join(parent_dir, d)) for d in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, d))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simulation_dirs = get_absolute_paths(\"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated\")\\nout_base_dir = \"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/train/static\"\\ngenerate_tomograms(out_base_dir,simulation_dirs, n_tomos=5)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def generate_tomograms(out_base_dir, simulation_dirs, n_tomos=5, tilt_range=(-60, 60, 3), \n",
    "                       detector_snr=[1.0, 2.0], simulation_threshold=float(\"inf\"), save_micrographs=False):\n",
    "    simulation_index = 0\n",
    "    for sim_dir in sorted(simulation_dirs):\n",
    "        if simulation_index > simulation_threshold:\n",
    "            break\n",
    "        # Check if the simulation directory exists\n",
    "        if not os.path.exists(sim_dir):\n",
    "            print(f\"Simulation directory {sim_dir} does not exist. Skipping.\")\n",
    "            continue\n",
    "        # Check if the simulation directory is empty\n",
    "        if not os.listdir(sim_dir):\n",
    "            print(f\"Simulation directory {sim_dir} is empty. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Processing simulation directory: {sim_dir}\")\n",
    "        tom_dir = os.path.join(sim_dir, \"tomos\")\n",
    "        if not os.path.exists(sim_dir):\n",
    "            raise FileNotFoundError(\"Tomogram directory not found.\")\n",
    "        sys.path.append(os.path.dirname(sim_dir))\n",
    "        \n",
    "        tem_dir = os.path.join(sim_dir, \"tem\")\n",
    "        if not os.path.exists(tem_dir):\n",
    "            raise FileNotFoundError(\"TEM directory not found.\") \n",
    "        for tomod_id in range(n_tomos):\n",
    "            print(\"GENERATING TOMOGRAM NUMBER:\", tomod_id)\n",
    "            hold_time = time.time()\n",
    "            # Create a separate output directory for each tomogram inside the simulation directory\n",
    "            tomo_output_dir = os.path.join(out_base_dir, \"ExperimentRuns\", f\"tomogram_{simulation_index}_{tomod_id}\")\n",
    "            os.makedirs(tomo_output_dir, exist_ok=True)\n",
    "            \n",
    "            # File paths\n",
    "            tomo_den_out = os.path.join(tom_dir, f\"tomo_den_{tomod_id}.mrc\")\n",
    "            tomo_lbls_out = os.path.join(tom_dir, f\"tomo_lbls_{tomod_id}.mrc\")\n",
    "            poly_den_out = os.path.join(tom_dir, f\"poly_den_{tomod_id}.vtp\")\n",
    "            poly_skel_out = os.path.join(tom_dir, f\"poly_skel_{tomod_id}.vtp\")\n",
    "            \n",
    "            if not all(os.path.exists(f) for f in [tomo_den_out, tomo_lbls_out, poly_den_out, poly_skel_out]):\n",
    "                raise FileNotFoundError(\"One or more required input files are missing.\")\n",
    "            \n",
    "            synth_tomo = SynthTomo()\n",
    "            synth_tomo.set_den(tomo_den_out)\n",
    "            synth_tomo.set_poly(poly_den_out)\n",
    "            \n",
    "            # TEM for 3D reconstructions\n",
    "            temic = tem.TEM(tem_dir)\n",
    "            vol = lio.load_mrc(tomo_den_out)\n",
    "            temic.gen_tilt_series_imod(vol, np.arange(*tilt_range), ax=\"Y\")\n",
    "            temic.add_mics_misalignment(1, 1.5, 0.2)\n",
    "            \n",
    "            # Apply SNR noise\n",
    "            if detector_snr:\n",
    "                snr = round(random.uniform(detector_snr[0], detector_snr[1]), 2) if len(detector_snr) > 1 else detector_snr[0]\n",
    "                temic.add_detector_noise(snr)\n",
    "            \n",
    "            temic.invert_mics_den()\n",
    "            temic.set_header(data=\"mics\", p_size=(10, 10, 10))\n",
    "            temic.recon3D_imod()\n",
    "            temic.set_header(data=\"rec3d\", p_size=(10, 10, 10), origin=(0, 0, 0))\n",
    "            \n",
    "            out_mics = os.path.join(tomo_output_dir, f\"tomo_mics_{tomod_id}_snr{snr}.mrc\")\n",
    "            out_tomo_rec = os.path.join(tomo_output_dir, f\"tomo_rec_{tomod_id}_snr{snr}.mrc\")\n",
    "            \n",
    "            # Conditionally save the micrographs\n",
    "            if save_micrographs:\n",
    "                shutil.copyfile(os.path.join(tem_dir, \"out_micrographs.mrc\"), out_mics)\n",
    "            \n",
    "            shutil.copyfile(os.path.join(tem_dir, \"out_rec3d.mrc\"), out_tomo_rec)\n",
    "            \n",
    "            synth_tomo.set_mics(out_mics if save_micrographs else None)\n",
    "            synth_tomo.set_tomo(out_tomo_rec)\n",
    "            \n",
    "            print(f\"Tomogram {tomod_id} processed in {time.time() - hold_time:.2f} seconds.\")\n",
    "        simulation_index += 1\n",
    "        print(f\"Simulation {simulation_index} processed.\")\n",
    "    print(\"Successfully terminated all simulations.\")\n",
    "\n",
    "\"\"\"simulation_dirs = get_absolute_paths(\"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated\")\n",
    "out_base_dir = \"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/train/static\"\n",
    "generate_tomograms(out_base_dir,simulation_dirs, n_tomos=5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quaternion_to_matrix(q1, q2, q3, q4):\n",
    "    \"\"\"Convert quaternion to a 4x4 transformation matrix.\"\"\"\n",
    "    # Normalize quaternion\n",
    "    norm = np.sqrt(q1**2 + q2**2 + q3**2 + q4**2)\n",
    "    q1, q2, q3, q4 = q1 / norm, q2 / norm, q3 / norm, q4 / norm\n",
    "\n",
    "    # Create rotation matrix\n",
    "    rotation_matrix = np.array([\n",
    "        [1 - 2*(q2**2 + q3**2), 2*(q1*q2 - q3*q4), 2*(q1*q3 + q2*q4), 0],\n",
    "        [2*(q1*q2 + q3*q4), 1 - 2*(q1**2 + q3**2), 2*(q2*q3 - q1*q4), 0],\n",
    "        [2*(q1*q3 - q2*q4), 2*(q2*q3 + q1*q4), 1 - 2*(q1**2 + q2**2), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    return rotation_matrix.tolist()\n",
    "\n",
    "def csv_to_json(csv_file, json_directory, labels_table):\n",
    "    # Read CSV file with pandas (using tab separator)\n",
    "    df = pd.read_csv(csv_file, sep='\\t')\n",
    "\n",
    "    # Read labels table to create a mapping from Code to protein names\n",
    "    labels_df = pd.read_csv(labels_table, sep='\\t')\n",
    "    code_to_protein = dict(zip(labels_df['LABEL'], labels_df['MODEL']))\n",
    "    get_name = lambda x: x.split(\"/\")[1].split(\".\")[0]\n",
    "    code_to_protein = {k: get_name(v) for k, v in code_to_protein.items()}  # Convert keys to strings\n",
    "    # Filter rows where Type is either 'SAWCL' or 'Mb-SAWLC'\n",
    "    df_filtered = df[df['Type'].isin(['SAWLC', 'Mb-SAWLC'])]\n",
    "\n",
    "    # Group by the 'Code' column to create separate JSON files for each protein\n",
    "    grouped = df_filtered.groupby('Label')\n",
    "\n",
    "    # Create the JSON directory if it doesn't exist\n",
    "    os.makedirs(json_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate over each group (protein code)\n",
    "    for label, group in grouped:\n",
    "        # Get the protein name from the mapping\n",
    "        protein_name = code_to_protein.get(label, str(label))  # Use code as fallback if not found\n",
    "\n",
    "        # Initialize JSON structure for this protein\n",
    "        json_data = {\n",
    "            \"pickable_object_name\": protein_name,  # Use the protein name\n",
    "            \"user_id\": \"curation\",\n",
    "            \"session_id\": \"0\",\n",
    "            \"run_name\": \"TS_5_4\",\n",
    "            \"voxel_spacing\": None,\n",
    "            \"unit\": \"angstrom\",\n",
    "            \"points\": [],\n",
    "            \"trust_orientation\": True\n",
    "        }\n",
    "\n",
    "        # Iterate over rows in the group\n",
    "        for _, row in group.iterrows():\n",
    "            # Extract relevant fields\n",
    "            x = float(row['X'])\n",
    "            y = float(row['Y'])\n",
    "            z = float(row['Z'])\n",
    "            instance_id = int(row['Label'])  # Use Label as instance_id\n",
    "            q1 = float(row['Q1'])\n",
    "            q2 = float(row['Q2'])\n",
    "            q3 = float(row['Q3'])\n",
    "            q4 = float(row['Q4'])\n",
    "\n",
    "            # Convert quaternion to transformation matrix\n",
    "            transformation = quaternion_to_matrix(q1, q2, q3, q4)\n",
    "\n",
    "            # Add point to JSON\n",
    "            json_data['points'].append({\n",
    "                \"location\": {\"x\": x, \"y\": y, \"z\": z},\n",
    "                \"transformation_\": transformation,\n",
    "                \"instance_id\": instance_id\n",
    "            })\n",
    "\n",
    "        # Define the output JSON file path\n",
    "        json_file = os.path.join(json_directory, f\"{protein_name}.json\")\n",
    "\n",
    "        # Write JSON to file\n",
    "        with open(json_file, mode='w') as file:\n",
    "            json.dump(json_data, file, indent=4)\n",
    "\n",
    "        #print(f\"Created JSON file for protein '{protein_name}' at {json_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(in_csv_list, out_dir, csv_dir_list,filter_types, labels_table):\n",
    "    \"\"\"\n",
    "    Main function to split a CSV file by density tomograms, filter by Type, and convert to JSON.\n",
    "\n",
    "    Parameters:\n",
    "        in_csv_list (list): List of paths to the input CSV files.\n",
    "        out_dir (str): Path to the output directory where split CSVs and JSON files will be saved.\n",
    "        csv_dir_list (list): List of directories where split CSVs will be stored.\n",
    "        filter_types (list): List of Type values to filter by.\n",
    "        labels_table (str): Path to the labels table CSV file.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    simulation_index = 0\n",
    "    for in_csv,csv_dir in zip(in_csv_list,csv_dir_list):\n",
    "        # Create a directory for the CSV files if it doesn't exist\n",
    "        os.makedirs(csv_dir, exist_ok=True)\n",
    "        # Load the input CSV file into a DataFrame\n",
    "        df = pd.read_csv(in_csv, sep='\\t')\n",
    "        df = df.drop(columns=['Tomo3D', 'Micrographs'], errors='ignore')\n",
    "        \n",
    "        # Group the DataFrame by the 'Density' column\n",
    "        grouped = df.groupby('Density')\n",
    "\n",
    "        # Iterate over each group and save to a separate CSV file\n",
    "        for density, group in tqdm(grouped):\n",
    "            density_csv = density.split(\"/\")[-1].split(\".\")[0]\n",
    "            density = density.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "            # Check if the density is empty or None\n",
    "            if density == None or density == \"\":\n",
    "                continue\n",
    "            # Filter by 'Type' column if it exists\n",
    "            \"\"\"if 'Type' in group.columns:\n",
    "                group = group[group['Type'].isin(filter_types)]\"\"\"\n",
    "            # Construct the output file path\n",
    "            \n",
    "            csv_file_path = os.path.join(csv_dir, f'{density_csv}.csv')  # Specify the file name\n",
    "            group.to_csv(csv_file_path, sep='\\t', index=False)          \n",
    "            # Create a JSON directory for each CSV file    \n",
    "            json_output_dir = os.path.join(out_dir,\"ExperimentRuns\", f\"tomogram_{simulation_index}_{density}\",\"Picks\")\n",
    "            \n",
    "            csv_to_json(csv_file_path, json_output_dir, labels_table)\n",
    "        simulation_index += 1\n",
    "        print(f\"Simulation {simulation_index} processed.\")\n",
    "    print(\"Successfully terminated all simulations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tomos_motif_list_paths(master_dir):\n",
    "    \"\"\"\n",
    "    Get the absolute paths of 'tomos_motif_list.csv' from all subdirectories in a master directory.\n",
    "    \n",
    "    Parameters:\n",
    "        master_dir (str): Path to the master directory containing subdirectories.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of absolute paths to 'tomos_motif_list.csv' files in each subdirectory.\n",
    "    \"\"\"\n",
    "    motif_paths = []\n",
    "    \n",
    "    # Iterate through all subdirectories in the master directory\n",
    "    for subdir, _, files in os.walk(master_dir):\n",
    "        if 'tomos_motif_list.csv' in files:\n",
    "            # Construct the absolute path for the csv file\n",
    "            motif_paths.append(os.path.abspath(os.path.join(subdir, 'tomos_motif_list.csv')))\n",
    "    \n",
    "    return motif_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v1/tomos_motif_list.csv', '/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v2/tomos_motif_list.csv', '/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v3/tomos_motif_list.csv', '/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v4/tomos_motif_list.csv']\n",
      "['/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v4/csv', '/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v3/csv', '/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v2/csv', '/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v1/csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:11<00:00, 14.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1 processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:04<00:00, 12.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 2 processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:07<00:00, 13.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 3 processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:08<00:00, 13.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 4 processed.\n",
      "Successfully terminated all simulations.\n"
     ]
    }
   ],
   "source": [
    "in_csv_list = get_tomos_motif_list_paths(\"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated\")\n",
    "in_csv_list = sorted(in_csv_list)\n",
    "print(in_csv_list)\n",
    "out_dir = \"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/train/overlay\"\n",
    "csv_dir_list = get_absolute_paths(\"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated\")\n",
    "csv_dir_list = [os.path.join(d, \"csv\") for d in csv_dir_list]\n",
    "print(csv_dir_list)\n",
    "filter_types = ['SAWLC', 'Mb-SAWLC']\n",
    "labels_table = \"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/data_simulated/all_v1/labels_table.csv\"\n",
    "main(in_csv_list, out_dir, csv_dir_list,filter_types, labels_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def delete_tomo_mics_files(base_dir):\n",
    "    \"\"\"\n",
    "    Deletes files with the 'tomo_mics_...' naming structure in the given directory and its subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir (str): Path to the base directory containing tomogram directories.\n",
    "    \"\"\"\n",
    "    # Regular expression to match 'tomo_mics_...' file names\n",
    "    pattern = re.compile(r\"^tomo_mics_.*\\.mrc$\")\n",
    "\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            # Check if the file matches the pattern\n",
    "            if pattern.match(file):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "# Specify the base directory\n",
    "base_dir = \"/Users/yusufberkoruc/Desktop/Master_thesis/polnet/train/static/ExperimentRuns\"\n",
    "\n",
    "# Call the function\n",
    "delete_tomo_mics_files(base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
